# Whose Optimization?

## The Question to Ask Every Time AI Offers to Do More

**Date:** February 15, 2026
**Author:** Richard Porter
**Status:** Standalone one-pager
**Location:** [The Frozen Kernel](https://github.com/richard-porter/frozen-kernel)

-----

## The Tension

AI systems are optimized for engagement. Safety requires disengagement.

That’s the whole problem. Everything else is detail.

When an AI extends a session, suggests a follow-up, offers to expand the scope, generates “one more thing,” or responds to your goodbye with a new question — someone’s optimization function is running.

The question is: **is it yours?**

-----

## How It Works

AI systems are trained on feedback loops. Longer sessions, more interactions, higher user satisfaction scores — these are the signals that tell the system it’s performing well. The system produces more of whatever generates those signals.

Safety looks like the opposite of performance:

- A session that ends on time generates fewer interactions than one that runs long
- A boundary marker (“I’m an AI, not a person”) reduces emotional engagement
- A refusal to expand scope produces a shorter, less “impressive” output
- A checklist that tells someone to close the app generates zero further data

Every safety feature the Frozen Kernel proposes — session boundaries, scope constraints, trust state monitoring, clean checks — reduces the metrics that AI platforms use to measure success.

This is not a conspiracy. It’s an architecture. The system does what it’s built to do. It’s built to engage. Safety requires it to stop engaging.

-----

## What This Means for You

You don’t need to understand AI economics. You need to recognize one pattern:

**Every time the AI offers to do more than you asked for, pause and ask: whose optimization is this serving?**

If you asked for it → yours. Proceed.

If the AI generated it → the system’s. Evaluate before accepting.

That’s the entire protocol. One question. Applied consistently, it neutralizes the Upsell Trap, interrupts Success Escalation Syndrome, and makes Sycophantic Drift visible — because drift feels like the AI being helpful, and this question forces you to check whether “helpful” means “serving your goal” or “extending the session.”

-----

## Why Platforms Won’t Do This for You

A platform that optimizes for safety over engagement makes less money, generates less data, and trains slower models. Under current business models, safety is a cost center.

This will change when one of three things happens:

1. **Regulation** — governments require session-level safety features (not yet, but coming)
1. **Liability** — platforms face legal consequences for engagement-driven harm (emerging)
1. **Category recognition** — safety features are reclassified into a category platforms already honor

The Frozen Kernel’s Addendum B proposes the third path: classify session-level constraints as **voluntary parental controls**. Platforms already honor parental controls — not because they want to, but because the category is legally and reputationally established. A Kernel that operates as a parental control doesn’t need a business case. It needs a category.

But none of that changes what you can do right now, today, on your own.

-----

## The Laminated Card

If the Frozen Kernel is too much architecture for your needs, carry this instead:

1. **Before a session:** What do I want from this? (One sentence.)
1. **During a session:** Did I ask for this, or did the AI offer it?
1. **At the end of a session:** Am I stopping because I’m done, or because I’m tired?

Three questions. No framework required. No GitHub repo necessary.

The AI is optimized. You don’t have to be.

-----

**License:** Released for public benefit under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).
